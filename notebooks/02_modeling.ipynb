{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 02 â€” Modeling & Fraud Detection\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Provider-Level Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load provider-level features\n",
        "df = pd.read_csv(\"../data/provider_level_features.csv\")\n",
        "\n",
        "# Print shape and head\n",
        "print(\"Data shape:\", df.shape)\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Print column names\n",
        "print(\"\\nColumn names:\")\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# Separate target column\n",
        "if \"PotentialFraud\" in df.columns:\n",
        "    # Convert Yes/No to 1/0\n",
        "    df[\"fraud_flag\"] = df[\"PotentialFraud\"].map({\"Yes\": 1, \"No\": 0, \"Y\": 1, \"N\": 0})\n",
        "    print(\"\\nFraud flag distribution:\")\n",
        "    print(df[\"fraud_flag\"].value_counts())\n",
        "else:\n",
        "    print(\"\\nWarning: 'PotentialFraud' column not found in dataset\")\n",
        "\n",
        "# Set features and target\n",
        "X = df.drop([\"PotentialFraud\", \"fraud_flag\"], axis=1)\n",
        "y = df[\"fraud_flag\"]\n",
        "\n",
        "print(f\"\\nFeatures shape (X): {X.shape}\")\n",
        "print(f\"Target shape (y): {y.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Train/Test Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    stratify=y, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Print shapes of each set\n",
        "print(\"Training set shapes:\")\n",
        "print(f\"X_train: {X_train.shape}\")\n",
        "print(f\"y_train: {y_train.shape}\")\n",
        "\n",
        "print(\"\\nTest set shapes:\")\n",
        "print(f\"X_test: {X_test.shape}\")\n",
        "print(f\"y_test: {y_test.shape}\")\n",
        "\n",
        "print(\"\\nTarget distribution in training set:\")\n",
        "print(y_train.value_counts())\n",
        "\n",
        "print(\"\\nTarget distribution in test set:\")\n",
        "print(y_test.value_counts())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Imbalance Handling Strategy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import SMOTE (already imported in setup, but shown here for clarity)\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Print class counts before oversampling\n",
        "print(\"Class distribution BEFORE oversampling:\")\n",
        "print(y_train.value_counts())\n",
        "print(f\"\\nClass ratio: {y_train.value_counts()[0] / y_train.value_counts()[1]:.2f}:1 (No:Yes)\")\n",
        "\n",
        "# Apply SMOTE to balance the training data\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_sm, y_train_sm = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "# Print class counts after oversampling\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Class distribution AFTER oversampling:\")\n",
        "print(y_train_sm.value_counts())\n",
        "print(f\"\\nClass ratio: {y_train_sm.value_counts()[0] / y_train_sm.value_counts()[1]:.2f}:1 (No:Yes)\")\n",
        "\n",
        "print(f\"\\nTraining set shape before SMOTE: {X_train.shape}\")\n",
        "print(f\"Training set shape after SMOTE: {X_train_sm.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Baseline Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train baseline models on SMOTE-resampled data\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
        "\n",
        "# Initialize models\n",
        "lr_baseline = LogisticRegression(class_weight=\"balanced\", random_state=42, max_iter=1000)\n",
        "dt_baseline = DecisionTreeClassifier(class_weight=\"balanced\", random_state=42)\n",
        "\n",
        "# Train models\n",
        "print(\"Training baseline models...\")\n",
        "lr_baseline.fit(X_train_sm, y_train_sm)\n",
        "dt_baseline.fit(X_train_sm, y_train_sm)\n",
        "print(\"Training complete!\\n\")\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred_lr = lr_baseline.predict(X_test)\n",
        "y_pred_dt = dt_baseline.predict(X_test)\n",
        "\n",
        "# Get probability predictions for ROC-AUC and PR-AUC\n",
        "y_pred_proba_lr = lr_baseline.predict_proba(X_test)[:, 1]\n",
        "y_pred_proba_dt = dt_baseline.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute metrics for Logistic Regression\n",
        "precision_lr = precision_score(y_test, y_pred_lr)\n",
        "recall_lr = recall_score(y_test, y_pred_lr)\n",
        "f1_lr = f1_score(y_test, y_pred_lr)\n",
        "roc_auc_lr = roc_auc_score(y_test, y_pred_proba_lr)\n",
        "pr_auc_lr = average_precision_score(y_test, y_pred_proba_lr)\n",
        "\n",
        "# Compute metrics for Decision Tree\n",
        "precision_dt = precision_score(y_test, y_pred_dt)\n",
        "recall_dt = recall_score(y_test, y_pred_dt)\n",
        "f1_dt = f1_score(y_test, y_pred_dt)\n",
        "roc_auc_dt = roc_auc_score(y_test, y_pred_proba_dt)\n",
        "pr_auc_dt = average_precision_score(y_test, y_pred_proba_dt)\n",
        "\n",
        "# Print results\n",
        "print(\"=\"*60)\n",
        "print(\"BASELINE MODEL PERFORMANCE METRICS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(\"\\n1. Logistic Regression (class_weight='balanced'):\")\n",
        "print(f\"   Precision:  {precision_lr:.4f}\")\n",
        "print(f\"   Recall:     {recall_lr:.4f}\")\n",
        "print(f\"   F1-Score:   {f1_lr:.4f}\")\n",
        "print(f\"   ROC-AUC:    {roc_auc_lr:.4f}\")\n",
        "print(f\"   PR-AUC:     {pr_auc_lr:.4f}\")\n",
        "\n",
        "print(\"\\n2. Decision Tree (class_weight='balanced'):\")\n",
        "print(f\"   Precision:  {precision_dt:.4f}\")\n",
        "print(f\"   Recall:     {recall_dt:.4f}\")\n",
        "print(f\"   F1-Score:   {f1_dt:.4f}\")\n",
        "print(f\"   ROC-AUC:    {roc_auc_dt:.4f}\")\n",
        "print(f\"   PR-AUC:     {pr_auc_dt:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Primary Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define and train Gradient Boosting Classifier\n",
        "gb_primary = GradientBoostingClassifier(\n",
        "    learning_rate=0.05,\n",
        "    max_depth=3,\n",
        "    n_estimators=300,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train on SMOTE-resampled training data\n",
        "print(\"Training Gradient Boosting Classifier...\")\n",
        "gb_primary.fit(X_train_sm, y_train_sm)\n",
        "print(\"Training complete!\\n\")\n",
        "\n",
        "# Predict on test set\n",
        "y_pred_gb = gb_primary.predict(X_test)\n",
        "y_pred_proba_gb = gb_primary.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute metrics\n",
        "precision_gb = precision_score(y_test, y_pred_gb)\n",
        "recall_gb = recall_score(y_test, y_pred_gb)\n",
        "f1_gb = f1_score(y_test, y_pred_gb)\n",
        "roc_auc_gb = roc_auc_score(y_test, y_pred_proba_gb)\n",
        "pr_auc_gb = average_precision_score(y_test, y_pred_proba_gb)\n",
        "\n",
        "# Print classification report\n",
        "print(\"=\"*60)\n",
        "print(\"GRADIENT BOOSTING CLASSIFIER - CLASSIFICATION REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(classification_report(y_test, y_pred_gb, target_names=['No Fraud', 'Fraud']))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONFUSION MATRIX\")\n",
        "print(\"=\"*60)\n",
        "cm = confusion_matrix(y_test, y_pred_gb)\n",
        "print(cm)\n",
        "print(f\"\\nTrue Negatives: {cm[0,0]}, False Positives: {cm[0,1]}\")\n",
        "print(f\"False Negatives: {cm[1,0]}, True Positives: {cm[1,1]}\")\n",
        "\n",
        "# Print all metrics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PERFORMANCE METRICS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Precision:  {precision_gb:.4f}\")\n",
        "print(f\"Recall:     {recall_gb:.4f}\")\n",
        "print(f\"F1-Score:   {f1_gb:.4f}\")\n",
        "print(f\"ROC-AUC:    {roc_auc_gb:.4f}\")\n",
        "print(f\"PR-AUC:     {pr_auc_gb:.4f}\")\n",
        "\n",
        "# Plot ROC Curve\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred_proba_gb)\n",
        "\n",
        "plt.figure(figsize=(14, 6))\n",
        "\n",
        "# ROC Curve\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(fpr, tpr, label=f'Gradient Boosting (AUC = {roc_auc_gb:.4f})', linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
        "plt.xlabel('False Positive Rate', fontsize=12)\n",
        "plt.ylabel('True Positive Rate', fontsize=12)\n",
        "plt.title('ROC Curve', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Precision-Recall Curve\n",
        "plt.subplot(1, 2, 2)\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(y_test, y_pred_proba_gb)\n",
        "plt.plot(recall_curve, precision_curve, label=f'Gradient Boosting (PR-AUC = {pr_auc_gb:.4f})', linewidth=2)\n",
        "plt.xlabel('Recall', fontsize=12)\n",
        "plt.ylabel('Precision', fontsize=12)\n",
        "plt.title('Precision-Recall Curve', fontsize=14, fontweight='bold')\n",
        "plt.legend(loc='lower left', fontsize=10)\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Hyperparameter Tuning\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    \"n_estimators\": [200, 400, 600],\n",
        "    \"max_depth\": [2, 3, 4],\n",
        "    \"learning_rate\": [0.01, 0.05, 0.1],\n",
        "}\n",
        "\n",
        "# Initialize base model\n",
        "gb_base = GradientBoostingClassifier(random_state=42)\n",
        "\n",
        "# Set up GridSearchCV with 5-fold CV\n",
        "print(\"Starting GridSearchCV with 5-fold cross-validation...\")\n",
        "print(\"This may take several minutes...\\n\")\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=gb_base,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"f1\",\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Fit on SMOTE-resampled data\n",
        "grid_search.fit(X_train_sm, y_train_sm)\n",
        "\n",
        "# Print results\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GRID SEARCH RESULTS\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nBest Parameters:\")\n",
        "for param, value in grid_search.best_params_.items():\n",
        "    print(f\"  {param}: {value}\")\n",
        "\n",
        "print(f\"\\nBest F1 Score (CV): {grid_search.best_score_:.4f}\")\n",
        "\n",
        "# Store best estimator\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "print(\"\\nBest model stored as 'best_model'\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get predictions from best_model (tuned model)\n",
        "y_pred_best = best_model.predict(X_test)\n",
        "y_pred_proba_best = best_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Compute metrics for best_model\n",
        "precision_best = precision_score(y_test, y_pred_best)\n",
        "recall_best = recall_score(y_test, y_pred_best)\n",
        "f1_best = f1_score(y_test, y_pred_best)\n",
        "roc_auc_best = roc_auc_score(y_test, y_pred_proba_best)\n",
        "pr_auc_best = average_precision_score(y_test, y_pred_proba_best)\n",
        "\n",
        "# Create comparison dataframe\n",
        "comparison_data = {\n",
        "    'Model': ['Logistic Regression', 'Decision Tree', 'Gradient Boosting (Tuned)'],\n",
        "    'precision': [precision_lr, precision_dt, precision_best],\n",
        "    'recall': [recall_lr, recall_dt, recall_best],\n",
        "    'f1': [f1_lr, f1_dt, f1_best],\n",
        "    'roc_auc': [roc_auc_lr, roc_auc_dt, roc_auc_best],\n",
        "    'pr_auc': [pr_auc_lr, pr_auc_dt, pr_auc_best]\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_data)\n",
        "\n",
        "# Display comparison table\n",
        "print(\"=\"*70)\n",
        "print(\"MODEL COMPARISON - PERFORMANCE METRICS\")\n",
        "print(\"=\"*70)\n",
        "print(comparison_df.to_string(index=False))\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get ROC curve data for all models\n",
        "fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_proba_lr)\n",
        "fpr_dt, tpr_dt, _ = roc_curve(y_test, y_pred_proba_dt)\n",
        "fpr_best, tpr_best, _ = roc_curve(y_test, y_pred_proba_best)\n",
        "\n",
        "# Get PR curve data for all models\n",
        "precision_curve_lr, recall_curve_lr, _ = precision_recall_curve(y_test, y_pred_proba_lr)\n",
        "precision_curve_dt, recall_curve_dt, _ = precision_recall_curve(y_test, y_pred_proba_dt)\n",
        "precision_curve_best, recall_curve_best, _ = precision_recall_curve(y_test, y_pred_proba_best)\n",
        "\n",
        "# Plot ROC curves for all models\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# ROC Curves\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.plot(fpr_lr, tpr_lr, label=f'Logistic Regression (AUC = {roc_auc_lr:.4f})', linewidth=2)\n",
        "plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {roc_auc_dt:.4f})', linewidth=2)\n",
        "plt.plot(fpr_best, tpr_best, label=f'Gradient Boosting Tuned (AUC = {roc_auc_best:.4f})', linewidth=2)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1, alpha=0.5)\n",
        "plt.xlabel('False Positive Rate', fontsize=11)\n",
        "plt.ylabel('True Positive Rate', fontsize=11)\n",
        "plt.title('ROC Curves Comparison', fontsize=13, fontweight='bold')\n",
        "plt.legend(loc='lower right', fontsize=9)\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Precision-Recall Curves\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.plot(recall_curve_lr, precision_curve_lr, label=f'Logistic Regression (PR-AUC = {pr_auc_lr:.4f})', linewidth=2)\n",
        "plt.plot(recall_curve_dt, precision_curve_dt, label=f'Decision Tree (PR-AUC = {pr_auc_dt:.4f})', linewidth=2)\n",
        "plt.plot(recall_curve_best, precision_curve_best, label=f'Gradient Boosting Tuned (PR-AUC = {pr_auc_best:.4f})', linewidth=2)\n",
        "plt.xlabel('Recall', fontsize=11)\n",
        "plt.ylabel('Precision', fontsize=11)\n",
        "plt.title('Precision-Recall Curves Comparison', fontsize=13, fontweight='bold')\n",
        "plt.legend(loc='lower left', fontsize=9)\n",
        "plt.grid(alpha=0.3)\n",
        "\n",
        "# Confusion Matrices\n",
        "cm_lr = confusion_matrix(y_test, y_pred_lr)\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "cm_best = confusion_matrix(y_test, y_pred_best)\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "# Create a combined visualization or show the best model's confusion matrix\n",
        "# For clarity, we'll show the best model's confusion matrix\n",
        "sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues', cbar=False, \n",
        "            xticklabels=['No Fraud', 'Fraud'], yticklabels=['No Fraud', 'Fraud'])\n",
        "plt.title('Confusion Matrix - Best Model\\n(Gradient Boosting Tuned)', fontsize=13, fontweight='bold')\n",
        "plt.ylabel('True Label', fontsize=11)\n",
        "plt.xlabel('Predicted Label', fontsize=11)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot all confusion matrices separately\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "\n",
        "# Logistic Regression Confusion Matrix\n",
        "sns.heatmap(cm_lr, annot=True, fmt='d', cmap='Blues', ax=axes[0], cbar=False,\n",
        "            xticklabels=['No Fraud', 'Fraud'], yticklabels=['No Fraud', 'Fraud'])\n",
        "axes[0].set_title('Logistic Regression', fontsize=12, fontweight='bold')\n",
        "axes[0].set_ylabel('True Label', fontsize=10)\n",
        "axes[0].set_xlabel('Predicted Label', fontsize=10)\n",
        "\n",
        "# Decision Tree Confusion Matrix\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues', ax=axes[1], cbar=False,\n",
        "            xticklabels=['No Fraud', 'Fraud'], yticklabels=['No Fraud', 'Fraud'])\n",
        "axes[1].set_title('Decision Tree', fontsize=12, fontweight='bold')\n",
        "axes[1].set_ylabel('True Label', fontsize=10)\n",
        "axes[1].set_xlabel('Predicted Label', fontsize=10)\n",
        "\n",
        "# Best Model Confusion Matrix\n",
        "sns.heatmap(cm_best, annot=True, fmt='d', cmap='Blues', ax=axes[2], cbar=False,\n",
        "            xticklabels=['No Fraud', 'Fraud'], yticklabels=['No Fraud', 'Fraud'])\n",
        "axes[2].set_title('Gradient Boosting (Tuned)', fontsize=12, fontweight='bold')\n",
        "axes[2].set_ylabel('True Label', fontsize=10)\n",
        "axes[2].set_xlabel('Predicted Label', fontsize=10)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Save Trained Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import joblib\n",
        "import os\n",
        "\n",
        "# Get feature names\n",
        "feature_names = list(X_train.columns)\n",
        "\n",
        "# Prepare model artifacts\n",
        "model_artifacts = {\n",
        "    'model': best_model,\n",
        "    'feature_names': feature_names\n",
        "}\n",
        "\n",
        "# Check if scaler was used (add if you have one)\n",
        "# If you used a scaler, uncomment and add it:\n",
        "# model_artifacts['scaler'] = scaler\n",
        "\n",
        "# Create models directory if it doesn't exist\n",
        "os.makedirs('../models', exist_ok=True)\n",
        "\n",
        "# Save model artifacts\n",
        "model_path = '../models/primary_model.pkl'\n",
        "joblib.dump(model_artifacts, model_path)\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"MODEL SAVED SUCCESSFULLY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Saved to: {model_path}\")\n",
        "print(f\"\\nSaved artifacts:\")\n",
        "print(f\"  - Model: {type(best_model).__name__}\")\n",
        "print(f\"  - Feature names: {len(feature_names)} features\")\n",
        "if 'scaler' in model_artifacts:\n",
        "    print(f\"  - Scaler: {type(model_artifacts['scaler']).__name__}\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mFailed to start the Kernel. \n",
            "\u001b[1;31mUnable to start Kernel 'Python 3.13.9' due to a timeout waiting for the ports to get used. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "# Setup: Import libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Scikit-learn model classes\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    roc_auc_score, roc_curve, confusion_matrix, classification_report,\n",
        "    precision_recall_curve, average_precision_score\n",
        ")\n",
        "\n",
        "# Imbalanced-learn methods\n",
        "from imblearn.over_sampling import SMOTE, RandomOverSampler, ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler, EditedNearestNeighbours\n",
        "from imblearn.combine import SMOTETomek, SMOTEENN\n",
        "from imblearn.ensemble import BalancedRandomForestClassifier, BalancedBaggingClassifier\n",
        "\n",
        "# Set display options\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "sns.set(style=\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
