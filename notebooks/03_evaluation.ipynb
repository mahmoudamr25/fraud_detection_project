{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 - Evaluation: Model Comparison and Class Imbalance Handling\n",
        "\n",
        "**Member 3 - Modeling & Class Imbalance**\n",
        "\n",
        "This notebook focuses on:\n",
        "- Training multiple classification models (Logistic Regression, Decision Tree, Random Forest, Gradient Boosting, SVM)\n",
        "- Handling class imbalance using `class_weight=\"balanced\"` instead of resampling\n",
        "- Comprehensive model evaluation using ROC-AUC, PR-AUC, and classification metrics\n",
        "- Identifying the best model for fraud detection based on fraud class recall and PR-AUC\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import joblib\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix, classification_report, roc_auc_score, \n",
        "    average_precision_score, roc_curve, precision_recall_curve,\n",
        "    precision_recall_fscore_support\n",
        ")\n",
        "\n",
        "# Set display options\n",
        "pd.set_option(\"display.max_columns\", 200)\n",
        "pd.set_option(\"display.max_rows\", 100)\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Load Provider-Level Dataset\n",
        "\n",
        "We load the provider-level features created by Member 2. This dataset contains one row per provider with aggregated features and the fraud label.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if provider_level_features.csv exists, if not create a basic version\n",
        "provider_features_path = \"../data/provider_level_features.csv\"\n",
        "\n",
        "if not os.path.exists(provider_features_path):\n",
        "    print(\"provider_level_features.csv not found. Creating basic version from raw data...\")\n",
        "    \n",
        "    # Load raw data files\n",
        "    labels_df = pd.read_csv(\"../data/Train-1542865627584.csv\")\n",
        "    inpatient_df = pd.read_csv(\"../data/Train_Inpatientdata-1542865627584.csv\")\n",
        "    outpatient_df = pd.read_csv(\"../data/Train_Outpatientdata-1542865627584.csv\")\n",
        "    \n",
        "    # Create basic provider-level aggregations\n",
        "    provider_features = []\n",
        "    \n",
        "    for provider in labels_df[\"Provider\"].unique():\n",
        "        provider_label = labels_df[labels_df[\"Provider\"] == provider][\"PotentialFraud\"].iloc[0]\n",
        "        \n",
        "        # Aggregate inpatient and outpatient claims\n",
        "        ip_provider = inpatient_df[inpatient_df[\"Provider\"] == provider]\n",
        "        op_provider = outpatient_df[outpatient_df[\"Provider\"] == provider]\n",
        "        \n",
        "        features = {\n",
        "            \"Provider\": provider,\n",
        "            \"PotentialFraud\": provider_label,\n",
        "            \"num_inpatient_claims\": len(ip_provider),\n",
        "            \"num_outpatient_claims\": len(op_provider),\n",
        "            \"total_claims\": len(ip_provider) + len(op_provider),\n",
        "        }\n",
        "        \n",
        "        # Add numeric aggregations if columns exist\n",
        "        numeric_cols_ip = ip_provider.select_dtypes(include=[np.number]).columns\n",
        "        numeric_cols_op = op_provider.select_dtypes(include=[np.number]).columns\n",
        "        \n",
        "        for col in numeric_cols_ip:\n",
        "            if col not in [\"Provider\"] and not ip_provider[col].isna().all():\n",
        "                features[f\"ip_{col}_mean\"] = ip_provider[col].mean()\n",
        "                features[f\"ip_{col}_sum\"] = ip_provider[col].sum()\n",
        "        \n",
        "        for col in numeric_cols_op:\n",
        "            if col not in [\"Provider\"] and not op_provider[col].isna().all():\n",
        "                features[f\"op_{col}_mean\"] = op_provider[col].mean()\n",
        "                features[f\"op_{col}_sum\"] = op_provider[col].sum()\n",
        "        \n",
        "        provider_features.append(features)\n",
        "    \n",
        "    df = pd.DataFrame(provider_features)\n",
        "    df.to_csv(provider_features_path, index=False)\n",
        "    print(f\"Created {provider_features_path} with {df.shape[0]} providers and {df.shape[1]} features\")\n",
        "else:\n",
        "    # Load existing provider-level features\n",
        "    df = pd.read_csv(provider_features_path)\n",
        "    print(f\"Loaded {provider_features_path}\")\n",
        "\n",
        "print(f\"\\nData shape: {df.shape}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(df.head())\n",
        "\n",
        "print(f\"\\nColumn names ({len(df.columns)} total):\")\n",
        "print(df.columns.tolist())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.1 Create Fraud Flag and Handle Label Column\n",
        "\n",
        "The label column `PotentialFraud` contains \"Y\" (fraud) and \"N\" (non-fraud). We convert this to a binary numeric flag.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create fraud_flag: 1 if PotentialFraud == \"Y\", else 0\n",
        "df[\"fraud_flag\"] = df[\"PotentialFraud\"].map({\"Y\": 1, \"N\": 0, \"Yes\": 1, \"No\": 0}).fillna(0).astype(int)\n",
        "\n",
        "# Drop the original PotentialFraud column (we have fraud_flag now)\n",
        "if \"PotentialFraud\" in df.columns:\n",
        "    df = df.drop(columns=[\"PotentialFraud\"])\n",
        "\n",
        "print(\"Fraud flag distribution:\")\n",
        "print(df[\"fraud_flag\"].value_counts(normalize=True))\n",
        "print(f\"\\nTotal samples: {len(df)}\")\n",
        "print(f\"Fraud cases: {df['fraud_flag'].sum()}\")\n",
        "print(f\"Non-fraud cases: {(df['fraud_flag'] == 0).sum()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Features vs Target\n",
        "\n",
        "We identify and drop ID columns, then separate features (X) from the target (y).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Identify ID columns to drop (columns containing \"provider\" or ending with \"id\")\n",
        "id_columns = []\n",
        "for col in df.columns:\n",
        "    col_lower = col.lower()\n",
        "    if \"provider\" in col_lower or col_lower.endswith(\"id\") or col_lower.endswith(\"_id\"):\n",
        "        if col != \"fraud_flag\":  # Don't drop the target\n",
        "            id_columns.append(col)\n",
        "\n",
        "print(\"ID columns to drop:\", id_columns)\n",
        "\n",
        "# Define target and features\n",
        "y = df[\"fraud_flag\"]\n",
        "X = df.drop(columns=[\"fraud_flag\"] + id_columns)\n",
        "\n",
        "print(f\"\\nX shape: {X.shape}\")\n",
        "print(f\"y shape: {y.shape}\")\n",
        "print(f\"\\nLabel distribution:\")\n",
        "print(y.value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train/Validation Split\n",
        "\n",
        "We split the data into training (80%) and validation (20%) sets, using stratification to maintain class balance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, \n",
        "    test_size=0.2, \n",
        "    stratify=y, \n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"Training set - X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
        "print(f\"Validation set - X_val: {X_val.shape}, y_val: {y_val.shape}\")\n",
        "\n",
        "print(f\"\\nFraud ratio in training set: {y_train.mean():.4f}\")\n",
        "print(f\"Fraud ratio in validation set: {y_val.mean():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Preprocessing\n",
        "\n",
        "We create a preprocessing pipeline that standardizes numeric features and one-hot encodes categorical features.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect numeric vs categorical columns\n",
        "numeric_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "categorical_cols = X_train.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "\n",
        "print(f\"Numeric columns: {len(numeric_cols)}\")\n",
        "print(f\"Categorical columns: {len(categorical_cols)}\")\n",
        "\n",
        "# Build ColumnTransformer\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), numeric_cols),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), categorical_cols)\n",
        "    ],\n",
        "    remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "print(\"\\nPreprocessor created successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Models to Train\n",
        "\n",
        "We define five models as sklearn Pipelines, each wrapped with the preprocessor. For imbalanced data, we use `class_weight=\"balanced\"` for models that support it. This adjusts class weights inversely proportional to class frequencies, which is more efficient than resampling techniques like SMOTE.\n",
        "\n",
        "**Why class_weight=\"balanced\" instead of resampling?**\n",
        "- More efficient: no need to duplicate or remove samples\n",
        "- Preserves original data distribution\n",
        "- Works well with tree-based and linear models\n",
        "- Avoids potential overfitting from synthetic samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define models as Pipelines\n",
        "models = {\n",
        "    \"Logistic Regression\": Pipeline(steps=[\n",
        "        (\"preprocess\", preprocessor),\n",
        "        (\"model\", LogisticRegression(\n",
        "            class_weight=\"balanced\",\n",
        "            max_iter=500,\n",
        "            solver=\"lbfgs\",\n",
        "            random_state=42\n",
        "        ))\n",
        "    ]),\n",
        "    \n",
        "    \"Decision Tree\": Pipeline(steps=[\n",
        "        (\"preprocess\", preprocessor),\n",
        "        (\"model\", DecisionTreeClassifier(\n",
        "            class_weight=\"balanced\",\n",
        "            random_state=42,\n",
        "            max_depth=None\n",
        "        ))\n",
        "    ]),\n",
        "    \n",
        "    \"Random Forest\": Pipeline(steps=[\n",
        "        (\"preprocess\", preprocessor),\n",
        "        (\"model\", RandomForestClassifier(\n",
        "            class_weight=\"balanced\",\n",
        "            n_estimators=300,\n",
        "            random_state=42,\n",
        "            n_jobs=-1\n",
        "        ))\n",
        "    ]),\n",
        "    \n",
        "    \"Gradient Boosting\": Pipeline(steps=[\n",
        "        (\"preprocess\", preprocessor),\n",
        "        (\"model\", GradientBoostingClassifier(\n",
        "            n_estimators=300,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=3,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ]),\n",
        "    \n",
        "    \"SVM RBF\": Pipeline(steps=[\n",
        "        (\"preprocess\", preprocessor),\n",
        "        (\"model\", SVC(\n",
        "            kernel=\"rbf\",\n",
        "            class_weight=\"balanced\",\n",
        "            probability=True,\n",
        "            random_state=42\n",
        "        ))\n",
        "    ])\n",
        "}\n",
        "\n",
        "print(f\"Defined {len(models)} models:\")\n",
        "for name in models.keys():\n",
        "    print(f\"  - {name}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Descriptions\n",
        "\n",
        "- **Logistic Regression**: A linear model that estimates the probability of fraud using a sigmoid function.\n",
        "- **Decision Tree**: A tree-based model that splits data based on feature values to classify fraud.\n",
        "- **Random Forest**: An ensemble of decision trees that votes on the final prediction, reducing overfitting.\n",
        "- **Gradient Boosting**: Sequentially builds trees that correct errors from previous trees, creating a strong predictive model.\n",
        "- **SVM RBF**: Uses a radial basis function kernel to find a non-linear decision boundary separating fraud from non-fraud.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Evaluation Helper Function\n",
        "\n",
        "This function trains a model, evaluates it on the validation set, and returns key metrics. For fraud detection, **PR-AUC and Recall for the fraud class are more important than overall accuracy** because:\n",
        "- PR-AUC focuses on the minority class (fraud), which is critical in imbalanced datasets\n",
        "- Recall measures how many actual fraud cases we catch - missing fraud is costly\n",
        "- Overall accuracy can be misleading when classes are imbalanced (e.g., 95% accuracy if we predict all non-fraud)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_model(name, model, X_train, y_train, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Train and evaluate a model, returning key metrics.\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Evaluating: {name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Fit the model\n",
        "    model.fit(X_train, y_train)\n",
        "    \n",
        "    # Get predictions and probabilities\n",
        "    y_pred = model.predict(X_val)\n",
        "    y_proba = model.predict_proba(X_val)[:, 1]\n",
        "    \n",
        "    # Compute metrics\n",
        "    cm = confusion_matrix(y_val, y_pred)\n",
        "    roc_auc = roc_auc_score(y_val, y_proba)\n",
        "    pr_auc = average_precision_score(y_val, y_proba)\n",
        "    \n",
        "    # Get precision, recall, f1 for fraud class (class 1)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(y_val, y_pred, pos_label=1, zero_division=0)\n",
        "    \n",
        "    # Print results\n",
        "    print(f\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "    \n",
        "    print(f\"\\nClassification Report:\")\n",
        "    print(classification_report(y_val, y_pred, digits=3))\n",
        "    \n",
        "    print(f\"\\nROC-AUC: {roc_auc:.4f}\")\n",
        "    print(f\"PR-AUC: {pr_auc:.4f}\")\n",
        "    \n",
        "    # Return metrics dictionary\n",
        "    return {\n",
        "        \"model\": name,\n",
        "        \"roc_auc\": roc_auc,\n",
        "        \"pr_auc\": pr_auc,\n",
        "        \"recall_fraud\": recall[1] if len(recall) > 1 else recall[0],\n",
        "        \"precision_fraud\": precision[1] if len(precision) > 1 else precision[0],\n",
        "        \"f1_fraud\": f1[1] if len(f1) > 1 else f1[0]\n",
        "    }\n",
        "\n",
        "print(\"Evaluation function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Fit All Models and Build Comparison Table\n",
        "\n",
        "We train all models and collect their performance metrics into a comparison table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train and evaluate all models\n",
        "results = []\n",
        "\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        result = evaluate_model(name, model, X_train, y_train, X_val, y_val)\n",
        "        results.append(result)\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError training {name}: {e}\")\n",
        "        print(\"Skipping this model...\")\n",
        "\n",
        "# Convert to DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Sort by PR-AUC (most important for fraud detection)\n",
        "results_df = results_df.sort_values(\"pr_auc\", ascending=False)\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(\"MODEL COMPARISON - PERFORMANCE METRICS\")\n",
        "print(f\"{'='*70}\")\n",
        "print(results_df.to_string(index=False))\n",
        "print(f\"{'='*70}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Plot ROC & PR Curves for Best Model\n",
        "\n",
        "We visualize the performance of the best model (highest PR-AUC) using ROC and Precision-Recall curves.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get best model name\n",
        "best_model_name = results_df.iloc[0][\"model\"]\n",
        "best_model = models[best_model_name]\n",
        "\n",
        "print(f\"Best model (by PR-AUC): {best_model_name}\")\n",
        "\n",
        "# Refit on training data\n",
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions for plotting\n",
        "y_proba_best = best_model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "# Compute ROC curve\n",
        "fpr, tpr, _ = roc_curve(y_val, y_proba_best)\n",
        "roc_auc_best = roc_auc_score(y_val, y_proba_best)\n",
        "\n",
        "# Compute PR curve\n",
        "precision_curve, recall_curve, _ = precision_recall_curve(y_val, y_proba_best)\n",
        "pr_auc_best = average_precision_score(y_val, y_proba_best)\n",
        "\n",
        "# Create plots\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "# ROC Curve\n",
        "axes[0].plot(fpr, tpr, label=f'{best_model_name} (AUC = {roc_auc_best:.4f})', linewidth=2)\n",
        "axes[0].plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1, alpha=0.5)\n",
        "axes[0].set_xlabel('False Positive Rate', fontsize=12)\n",
        "axes[0].set_ylabel('True Positive Rate', fontsize=12)\n",
        "axes[0].set_title('ROC Curve - Best Model', fontsize=14, fontweight='bold')\n",
        "axes[0].legend(loc='lower right', fontsize=10)\n",
        "axes[0].grid(alpha=0.3)\n",
        "\n",
        "# Precision-Recall Curve\n",
        "axes[1].plot(recall_curve, precision_curve, label=f'{best_model_name} (PR-AUC = {pr_auc_best:.4f})', linewidth=2)\n",
        "axes[1].set_xlabel('Recall', fontsize=12)\n",
        "axes[1].set_ylabel('Precision', fontsize=12)\n",
        "axes[1].set_title('Precision-Recall Curve - Best Model', fontsize=14, fontweight='bold')\n",
        "axes[1].legend(loc='lower left', fontsize=10)\n",
        "axes[1].grid(alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nBest Model Performance:\")\n",
        "print(f\"  ROC-AUC: {roc_auc_best:.4f}\")\n",
        "print(f\"  PR-AUC: {pr_auc_best:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Save Trained Models\n",
        "\n",
        "We save all trained models to the `models/` directory for future use.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create models directory if it doesn't exist\n",
        "os.makedirs(\"../models\", exist_ok=True)\n",
        "\n",
        "# Re-fit each model on the full dataset (X, y) and save\n",
        "print(\"Training models on full dataset and saving...\")\n",
        "\n",
        "model_files = {\n",
        "    \"Logistic Regression\": \"logistic_regression.pkl\",\n",
        "    \"Decision Tree\": \"decision_tree.pkl\",\n",
        "    \"Random Forest\": \"random_forest.pkl\",\n",
        "    \"Gradient Boosting\": \"gradient_boosting.pkl\",\n",
        "    \"SVM RBF\": \"svm_rbf.pkl\"\n",
        "}\n",
        "\n",
        "for name, model in models.items():\n",
        "    try:\n",
        "        # Fit on full dataset\n",
        "        model.fit(X, y)\n",
        "        \n",
        "        # Save model\n",
        "        if name in model_files:\n",
        "            filepath = f\"../models/{model_files[name]}\"\n",
        "            joblib.dump(model, filepath)\n",
        "            print(f\"  Saved: {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  Error saving {name}: {e}\")\n",
        "\n",
        "# Save comparison metrics\n",
        "results_df.to_csv(\"../models/model_comparison_metrics.csv\", index=False)\n",
        "print(f\"\\nSaved: ../models/model_comparison_metrics.csv\")\n",
        "\n",
        "print(\"\\nAll models saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Conclusion\n",
        "\n",
        "### Model Recommendation\n",
        "\n",
        "Based on the comparison metrics, the **best model for fraud detection** is the one with the highest PR-AUC and fraud class recall. PR-AUC is particularly important for imbalanced datasets because it focuses on the minority class (fraud cases) that we care most about detecting.\n",
        "\n",
        "### Trade-offs\n",
        "\n",
        "- **Random Forest vs Gradient Boosting**: Random Forest is faster to train and less prone to overfitting, while Gradient Boosting often achieves higher performance but requires more careful tuning and is slower.\n",
        "- **Linear models (Logistic Regression)**: Fast and interpretable, but may struggle with complex non-linear patterns in fraud detection.\n",
        "- **Decision Tree**: Simple and interpretable, but prone to overfitting; Random Forest addresses this by averaging multiple trees.\n",
        "- **SVM**: Can capture complex patterns but is computationally expensive and may not scale well to large datasets.\n",
        "\n",
        "### Future Improvements\n",
        "\n",
        "1. **Hyperparameter Tuning**: Use GridSearchCV or RandomizedSearchCV to optimize model parameters (e.g., n_estimators, max_depth, learning_rate for Gradient Boosting).\n",
        "2. **Feature Engineering**: Create additional provider-level features such as:\n",
        "   - Temporal patterns (seasonality, trends)\n",
        "   - Statistical features (coefficient of variation, skewness)\n",
        "   - Interaction features between claim types\n",
        "3. **Ensemble Methods**: Combine predictions from multiple models using voting or stacking to improve robustness.\n",
        "4. **Cost-Sensitive Learning**: Adjust the decision threshold based on the cost of false positives vs false negatives in the fraud detection context.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
